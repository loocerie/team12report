{
  "name": "Beaver Works Summer Institute: Team 12 Technical Report",
  "tagline": "",
  "body": "Project Overview\r\nThe purpose of this class was to educate students about programming autonomous robots using ROS (Robot Operating System) and Python software. The instructors wanted students to feel comfortable with their technical skills so that they could enjoy this camp and pursue further educational opportunities with clear fundamentals. Students hope to spread their knowledge and help others transition into a STEM disciple.\r\nInstructors taught students how to use the robot’s laser sensor to detect range from obstacles, camera to identify the color of obstacles, odometry to perceive the robot’s movement, and a combination of these sensors to determine where it is in a map. Not only did they learn the theory behind these concepts from MIT and JPL staff, they were able to implement it on robots by following instructions on labs and learn from a trial and error basis.  \r\nStudents were also able to experience seminar talks about cutting-edge technology from experts in their fields daily. They learnt about topics ranging from designing hackathons to creating mini-satellites to even getting some insider tips into the admissions process at MIT!\r\n\r\nWeek 1\r\nThe goal for week one was to become familiar with Linux and ROS, and then begin to control the driving of the racecar, eventually leading to a wall-following program. We began by learning about basic Linux commands and ROS structure. We familiarized ourselves with Linux commands to create directories and navigate through files, and learned about the modularity of ROS. \r\nModularity is an important aspect of ROS because it enables the user to use nodes and libraries created by others without fully understanding the details. A single task is divided into smaller, simpler tasks, each accomplished by a node, which work together to accomplish the larger goal. The modularity of ROS enables members of a team to work independently and simultaneously at first, each on a different node, and then join together after to put all the nodes together. We learned how to create ROS nodes, including publishers and subscribers, allowing the nodes to communicate and interact with each other. \r\n\tRobotic control systems were the main theme of this week. We learned about open-loop versus closed-loop controls, and about bang-bang and PID controls. In an open loop system, the system’s output does not affect later outputs. In a closed-loop system, however, the system’s output is fed back into sensors, which send these values back to the controller and change future outputs. The robot uses sensors to learn about its own current state, or a state estimation. In order to make a wall follower algorithm, we would need to take data from sensors on the racecar, specifically the lidar, and use this data to change the racecar’s behavior. Lidar, or laser radar, is a sensor that sends out lasers and detects the returning laser signals in order to calculate distance. The Lidar on the racecar extends 270 degrees around the car, from right to left, and returns a list of 1080 data points for distance, 4 for each degree. \r\nWe began with a bang-bang algorithm to make the racecar wall follow. Bang-bang control the simplest type of control, enabling the system to only alternate between two states. There is a set threshold, and if the error is outside of the threshold, some action is done to increase or decrease the error. Bang-bang control, while it is simple, is not usually the best type of control because the system is constantly switching between two extremes, and thus will oscillate significantly depending on the starting state estimation. \r\nWe used the race car's lidar data and took the minimum distance on the side of the wall the racecar is supposed to follow. Using this distance, we calculated our error (error = d_desired - distance) and compared this error to our set threshold. If the absolute value of error was less than the threshold , then the racecar would continue to drive straight. If not, the racecar would either steer right or left in order to maintain its error within the threshold. \r\n\tWe then improved our bang-bang controller to use an average distance instead of the minimum distance from the lidar. We took a small sector of the lidar data and then found the average of those distances to get a new estimated distance, which we then based our error on and used to control the racecar’s steering. \r\n\tA PID controller is another closed-loop control system similar to bang-bang, but more sophisticated so that the car does not oscillate as much. PID stands for Proportional Integral Derivative, which shows how the control system accounts for past, present, and future error values. A PID controller is more effective and refined than a bang-bang controller because it has three coefficient variables, Kp (proportional), Ki (integral), and Kd (derivative). It adjusts the system’s response based on how large or small the error is, how strong previous outputs were, and the current rate of error change. \r\n\tWe began developing a more sophisticated control system by starting with just the Kp coefficient. The addition of this coefficient makes the system output proportional to the error. If the error is large, the racecar steering is also large, if the error is small, the racecar steering is small. This enables the racecar to get as close to equilibrium as possible, as opposed to constantly oscillating between two extremes. We later added a Kd coefficient to make the wall follower even more effective, as it would alter the output to match previous outputs. \r\n\tDuring the challenge, our team’s racecar successfully wall followed without crashing, but we did not have the fastest time. Especially when put to the far right or left, the racecar would create large oscillations in trying to balance its error within the threshold. Our Kp and Kd values were not refined enough to minimize these wide oscillations. Even though it was not the most efficient, at least our wall follower worked. After all, having a safe car is much more important than having a fast car. \r\n\r\nWeek 2\r\nTechnical Goals: This week we hoped to be comfortable with the basic software and therefore program more. We wanted to get better at image processing, \r\nProcess: We started the week by learning about the opencv software, which allows the ZED Camera to read images and the processor to process images and extract meaningful information such as color, size, etc. We learnt how to convert from the RGB color system (one we are familiar with) to the HSV color system (one ideal for the computer). Some functions we used were the inRange function which gave us all the pixels which were within our bounds and bitwise_and which allowed the image’s selected colors to be published. We then worked with the ZED Camera and recorded its data using rosbag. We modified given code to detect and distinguish a red light from a green light.\r\nOn the next day, we created a publisher and subscriber node for the images of the ZED Camera. The subscriber node subscribed to images sent on the /camera/rgb/image_rect_color topic and the publisher node published image messages to  image_echo. \r\nOn Wednesday, we had to design a control system that would take in information from the ZED Camera to stop in front of the green blob. We had to make two nodes. The first one would detect the green blob by converting the zed camera image to something that ROS can use. Then, we make a mask (which separates the green bits from other color bits we don’t need).We use function that filter out noise such as Gaussian Blur. After that, we draw contours around the largest mask, and we follow the contour. Once the mask reaches a certain pixel size, we’d know whether we were close enough and then we would stop.\r\nOn Thursday and Friday, we had to design a robot that could follow either the left or right wall based on the color it sees. To do this, we first saw the blob and recognized whether it was green or blue. Then, we used a P controller to navigate towards it. Once we got close enough to the blob, we turned right or left (based on the color) and switched to our wall following program.\r\nOur robot was unsuccessful because we did not develop a reliable program which could transition between the node that went towards the blob and the node that started following the wall. Moreover, our robot could not always find the proper blob because it was often confused by the colors of the clothing the spectators were wearing and confused by the colors of the flag.\r\nTo try to fix these errors, we increased the threshold for the pixel length of the blob and narrowed down the color ranges so that it would only find the blob. To add a quick transition between programs, we modified our wall following program to turn right if x was pressed and no wall was found, and to turn left if a was pressed and no wall was found. Once the wall was found, we would go back to our PD controlled wall follower. However, we did not get time to test these changes.\r\n\r\nWeek 3\r\nWeek 3, while not the capstone week, introduced higher level autonomy while integrating the technologies taught in Weeks 1 and 2. We students were expected to use blob and color detection along with space exploration algorithms more complex than Week 1 wall-following to explore the given space. Localization and mapping - robot location sensing - was also a large focus of Week 3, and the technical lectures included path planning algorithms given a map of the robot space. \r\nMapping and localization, introduced toward the beginning of the week, is integral to autonomous robotics. Indeed, robot mobility requires knowledge of robot location. Autonomous robots, however, cannot accurately track their location and direction with mere GPS and compass; robots need to employ other sensing techniques to map their local environments and describe a path. This is where localization and mapping plays a role. Our RCs can use the lidar data detected from multiple positions along with knowledges of the distance and orientation of the RC itself (from the odometer) to map their environments. Instead of first mapping then localizing or vice versa, the robots use SLAM, or simultaneous localization and mapping, whereby localization occurs while the map is concurrently made. \r\nThe SLAM task at hand was simple: map the local race environment while also localizing the car with odometry. The RC used a built in program to run the SLAM algorithm, but the generated map lacked clarity and precision. The map seemed to fold in on itself like a pretzel, suggesting that the odometry measurements used to predict the direction and speed of the racecar were inaccurate. With further techniques, we could have perfected the map by accounting for odometry inaccuracies. \r\nPast localization and mapping, Week 3 also introduced vehicle mobility robotics such as path planning and obstacle avoidance algorithms. During the technical lectures, we learned about discretization of spaces and the application of A* and RQT algorithms, and during the lab, we implemented a variety of obstacle avoidance algorithms. The original obstacle avoidance algorithms written simply moved the car to the nearest “white space” available; the car would move towards the points with the farthest lidar readings. One major problem faced this week was the inaccuracies of certain Lidar measurements. Our group used an averaging algorithm to essentially smooth out the data generated by the lidar and prevent from inaccurate measurements. \r\nLater in the week, the potential fields method was introduced, whereby the vehicle is considered a “positive charge”, each particle is considered a “positive charge”, and the subsequent potential field is produced and analyzed. Following Winters’ explanation, our group derived a slightly more complex version of the algorithm giving us more control over vehicle turning. After much tuning, we produced an algorithm that could effectively avoid obstacles in a smooth, seamless manner. The robustness of this potential fields algorithm convinced us to use it again for the final race in Week 4 (see Week 4).\r\nThe final technical challenge in Week 3 was, in reality, a combination of Week 1 and Week 2: we were expected to explore the open space while simultaneously using image detection to detect certain colors. The key difference in image analysis between Week 2 and Week 3, however, was the new image detection requirement; we would earn more points detecting real images (that had been provided beforehand to the computer) within pink paper. Our group originally tackled this problem by computing the histograms of both the input image stored in the computer and the image from the ZED camera and comparing the two. However, we soon realized that the lighting conditions were subject to change, throwing off the histograms and skewing the image detection. Our team was unfortunately unable to solve this problem during Week 3, but we switched over to feature detection as opposed to histogram comparison during Week 4. \r\nOur final challenge ended up subpar, unfortunately - the team hit 8 obstacles, and we recognized merely 1 image. We soon learned after the race that our blue detection had been switched with the red detection (so even though we detected the colors, we didn’t correctly identify the colors), and upon further scrutiny, we realized that within the code, our HSV to BGR conversion had been mistakenly HSV to RGB (thus switching the red and the blue). This was a major blow to our team and our confidence, but we shook it off and prepared for the challenges in Week 4.\r\n\r\n\r\nWeek 4\r\nThe idea behind week 4 was to take all the concepts that we had learned in the previous 3 weeks and compile them into one project so that we could show off what was learned. A racetrack was set up in MIT’s Walker Memorial and we students were tasked with moving the robots along the track in an attempt to achieve the fastest time. \r\nDuring the first two days of Week 4 we students prepared for the technical challenge. On the first day our group prepared for the obstacle avoiding challenge, which was very similar to the one we did on Week 3. Our robot was supposed to go around an open space with obstacles and pick up as many blobs as possible without avoiding the obstacles. The first thing our group did was we re-wrote the code from week 3 over the weekends. It was very helpful because it made testing and debugging a lot easier; the new code also made it easier to add new color filters to detect new colors. By writing new codes, our group also discovered the BGR RGB mistake last week. The challenge of obstacle avoiding mission remained as our car continued to collide. Our solutions was to make the car less responsive to obstacles on the side. Our group tuned the constants in our potential field program and successfully avoided all the obstacles. \r\nOn Tuesday we had to deal with making a colored turn. Instead of hard coding the turn like in week 2, our group used a modified version of potential field to control the turn. When our car approached the wall, our code produced an ‘illusionary’ wall on one side of the car. The side was determined by the color we picked up on the wall. The car would then try to avoid the obstacles - the wall in front as well as the imaginary wall we created, and naturally it made a turn in our desired direction. \r\nOn Wednesday our group went to Walker Memorial and prepared for the final race. In the morning while people were setting up the race track our group worked on our codes. We delegated tasks into three parts: Normal racing, which focused on staying straight and avoiding the walls, Turning, which focused on making the correct turn at the shortcut intersection, and Controlling, which evaluated current conditions and made decisions on whether the car should use normal racing strategy or it should use shortcut handling strategy. The day didn’t go well for us, however. Every part of our code had problems. The racing strategy code could not make the car go in a straight line and avoid walls. The turning code made the car crash into the wall, and the Controlling code couldn’t switch from turning to racing at the right time. During the night our group decided to merge the racing and turning code, using the strategy of an imaginary wall like we used for our technical challenge. We quickly wrote a new code. \r\nOn Thursday we realized we were in a time crunch, but we still took the time and made a plan for the day. In the morning we focused on racing. We added a D controller to our potential field code, which adjusted steering angle based on how drastically the potential had been changing. The D controller was nice because whenever there was a drastic change it brought the steering angle down, which avoided the car constantly turning on a straight course. During the day we kept adjusting our control system, and by the end of the day our car could finish the race smoothly. We also made a team poster with a team name. \r\nOn Friday our group raced well. Despite having a lot of hardware issues our car had one of the strongest finishes as it finished all three of the time trial. \r\n\r\nVisual Support\r\n(everyone)\r\nUpload videos: Hayley\r\nEveryone else upload whatever photos/videos you have\r\nPictures of code maybe?\r\ncharts/diagrams/etc.\r\nVideos:\r\nhttps://www.youtube.com/watch?time_continue=5&v=TqN69NZ3m8w\r\nhttps://youtu.be/gV9Xs3kKbL4\r\nhttps://youtu.be/qHLDnnjNvj0\r\nhttps://youtu.be/8QlRD6RBav4\r\nhttps://youtu.be/Uelm8vr4BJE\r\nhttps://youtu.be/M3MSCNbEfdo\r\nhttps://youtu.be/5v-aQO6dczM\r\nhttps://youtu.be/kXoFbSFq4Q8\r\nhttps://youtu.be/fev-oOa7Edw\r\nhttps://youtu.be/LdqC4rhQdVY\r\n\r\nTechnical Conclusion\r\nSummarize results, goals, failures\r\n\tEvery group started out with the one goal of creating the robot racecar that can drive the fastest and smoothest through obstacles autonomously. Initially, this task seemed very complicated and frightening but over the course of four weeks, we gradually learned the concepts and skills needed to create the final racecar. We have had successes and failures along the way but we took them all in stride and ultimately learned a lot from them.\r\nDuring the first week, our goal was to program our race car to successfully drive parallel to a wall.  For the most part, we were pretty successful with our goal. Almost every team had their car following a wall very well although every team implemented their algorithm differently. However, we recognized that there were flaws in our algorithm. For example, when the car starts at a weird angle relative to the wall, our algorithm doesn’t work so well. We kept a note of this as something we might want to improve later on.\r\nDuring the second week, our goal was to utilize a blob detection algorithm to direct our racecar in a certain direction and then to use the wall following code from the prior week to drive the car down a certain path. Ultimately, our team wasn’t so successful because the week’s challenge incorporated many aspects that turned out to be rather confusing to integrate. While we were able to implement the two blob detection and wall following algorithms pretty well separately, integrating the two was a difficult process. \r\nDuring the third week, our goal was to program our race car to smoothly navigate around obstacles using the potential field algorithm. However, we had some issues with the car and so the potential field algorithm didn’t work as well as we had wanted to. However we think our issue this week is more a reflection of poor calibration than the algorithm itself. We did come out with a much better understanding of how to implement potential field on our racecar though.\r\n\r\nWhat we learned\r\n\tOn the technical side, we all learned a lot about how to work with ROS to create complex robots that interact among multiple nodes and programs. We learned about packages and workspaces and nodes and topics and we also improved our programming skills. Even though everyone came into this program with at least some basic knowledge of coding, we really learned how to apply coding and mathematics to solving real world problems. We learned how to use trigonometry to find distances between our race car and the wall, how to use RGB and HSV values to detect color, and how to use potential field to avoid obstacles. \r\n\tOn the other hand, we also learned how to work with each other to reach a common goal. Everyone in this program come from different states and different levels of experience at programming. In the beginning, it was difficult communicating with each other but through time and the communication workshops, we gradually learned how to effectively talk to each other. We learned how to give and receive feedback. We learned how to encourage and motivate each other. Etc etc. \r\n\t\r\nWhat we would do differently\r\n\tFrom a communications and teamwork standpoint, we would make it a bigger point to have group meetings prior to programming. We realize that not everyone is on the same skill level in terms of programming and ROS and by not having meetings, we risk leaving certain group members behind. We tried to do this on the last week and we found that it was really helpful in getting everyone on the same page. We also want to think through multiple algorithms to complete weekly tasks and the final race. Often times, we relied on just one algorithm but we realize that there may have been better methods that we didn’t explore. We would probably also make better use of our time by effectively splitting up the work between the team members. Sometimes we would have multiple people working on the same exact thing, which wasn’t a very productive use of our time. And I guess we could have just communicated more overall because even when we did split up the work, we’d have a hard time integrating everything because we did not effectively talk things through before hand.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nPersonal Reflections\r\n Nimish - Even though I didn’t have much experience with programming, I learned a lot, and I had a lot of fun! I hope to continue working on ROS and programming. In the future, I’ll spend more time making simple and robust algorithms that do not require much tuning because a lot of time was spent debugging. I’ll practice coding more by myself.\r\n\tNot only did I become better at technical skills, I improved my communications skills. Even though I thought making myself vulnerable is a sign of weakness, it is actually a strength that enables people to trust me and enable me to trust others.\r\n\tI made a lot of friends, both commuters and residential, and I’ll really miss everyone when we leave. I hope to have a reunion soon!\r\n\r\nLucy:\r\nWell for one thing, I feel honored to have been part of such an amazing group of students, teachers, and administrators in the Beaver Works Summer Program. This program has by far surpassed any expectations that I had coming in. Even though I had never done robotics prior to this, I found everything we did to be engaging and interesting. Initially, I wanted to participate in this summer program because I wanted to be exposed to different kinds of programming. Doing this program and learning about different kinds of programming has only solidified the positive feelings that I have towards coding and I know that computer science is definitely something that I want to continue doing.\r\nI don’t know if robotics itself is something that I want to pursue as a career in the future. I do think, though, that the future is very fast approaching a world where robots can play a very routine and integral part of our lives and having any preliminary knowledge about it will be beneficial. I look forward to being able to apply what I learned here to whatever I may decide to do in the future.\r\nOther than the technical stuff that we did, I was also pleasantly surprised by the other classes and seminars that we had throughout the program. Instead of solely focusing on the programming aspects, I liked learning about communication because it's something that everyone needs to know in order to exist alongside other people. On the same token, seminars gave me a good glimpse into the lives of different engineers, which helps with me figuring what I want to do in the future. \r\n\r\nHayley: \r\nI came into this program with almost no knowledge of robotics, but I still found it very interesting and exciting. I met many people who already had years of experience in programming and robotics, as well as people like me who had never done any robotics before. The mix of various experience levels was surprisingly helpful because we could all learn from each other. I learned a lot about ROS and OpenCV while also learning to become a better team member. I also really enjoyed the seminar series because it gave me the chance to learn about all different types of careers that are possible in the field of science and engineering. \r\n\r\nBhavik:\r\n\tThe Beaver Works Program was much like the Grand Prix racetrack: it was intense and complex, yet super fast. From the hours of gritty network debugging (and the instructors’ seemingly godlike solutions) to the hilarious communication sessions, the program was both intellectually enlightening and socially engaging- far better than anything I could have ever asked for. \r\n\tAs a high school student, all I’ve known is individual school work- learning from textbooks, completing projects, taking tests, the usual. Taking a wide variety of classes, I rarely get immediate hands on experience in a subject that aligns with my interests under the guidance of professors and instructors that are truly at the top of their field. I found all that and much more at the Beaver Works Program.\r\nMy waning days with the racecar involved tremendous personal growth, as a programmer, student, and friend. We programmed a racecar, learning new frameworks and operating systems and programming languages and libraries. We got a taste of a wide variety of potential career paths during the hour-long \r\n\r\nDistribution of work\r\nProject overview - Nimish\r\nWeek 1 - Hayley\r\nWeek 2 - Nimish\r\nWeek 3 - Bhavik\r\nWeek 4 - Will\r\nVisual support, photos/videos - everyone\r\nData, tables, diagrams, charts, etc. - Yifan\r\nTechnical conclusions - Lucy\r\nPersonal reflection - everyone\r\n\r\nOutline\r\nProject Overview\r\nDescribe class as a whole: lectures, various seminars, labs, communication classes, weekly challenges\r\nPurpose, components: learn about robotics and autonomy, collaboration and communication\r\nFour sections detailing your technical learning for each week\r\nWeek 1 - moving, wall-following, open loop control, bang bang, PID control\r\nTechnical goals:  Familiarize ourselves with ROS and write a simple wall-following program. Learn about control systems and state estimation, specifically bang-bang and PID control. \r\nApproach: each work on our own versions of wall following, then consult together and decide on the most effective version\r\nProcess: Began with a simple bang-bang controller, then advanced on to P and then PD control\r\nResults: wall following was effective most of the time except when starting very far to the right or to the left\r\nWeek 2 - image processing, color detection, blob detection\r\nTechnical goals: detect colored blobs and advance toward the blob, turn a certain direction based on the blob color\r\nApproach: one group of people develop blob detector that publishes blob location, size, and color. Another group of people develop visual servoing and turning node\r\nProcess: create node that detects and publishes blob location, size, and color. Another node that makes racecar visual servo to blob and then turn in a direction based on color\r\nResults: racecar visual servoed successfully to blob, turned in correct direction and wall followed until it encountered a pole, which disrupted the wall follower\r\nWeek 3 - localization, mapping, SLAM, obstacle avoiding, potential field\r\nTechnical goals: obstacle avoiding and detecting different colors\r\nApproach: one group works on potential field, another group refines HSV range values\r\nProcess: refine HSV hue values to detect various colors, create image comparison, potential field node that avoids obstacles, take pictures of blobs and publish the color\r\nResults: racecar avoided most obstacles and detected most blobs accurately, issue with RGB vs. BGR, mixed up red and blue\r\nWeek 4\r\nTechnical goals: win the challeng!\r\nApproach: distribute challenges among team mates, test at alternating times\r\nProcess: one group works on refining potential field node to be more efficient, one group works on improving color detector and image detector\r\nResults: \r\nVisual support for each section, as needed\r\nData tables\r\nFigures, diagrams, charts, photos\r\nVideo clips\r\nTechnical Conclusions\r\nSummarize results, goals, failures\r\nWhat we learned\r\nWhat we would/will do differently\r\nPersonal Reflections\r\nTeamwork, relationships, work in the future\r\nFuture improvements for program\r\n\r\nHow to explain data results\r\n1. Performance of visual detection: for instance, in what hue ranges did the vision system work?  What is the distribution of the accuracy?\r\n2. Performance of control systems: for instance, what is the average error, what is the distribution of the error?\r\n3. Performance of the overall system in terms of speed: What is the average speed throughout the track? What is its variance, distribution? How does this change when they change various parameters?\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}