{
  "name": "Beaver Works Summer Institute: Team 12 Technical Report",
  "tagline": "",
  "body": "# Project Overview\r\nThe purpose of this class was to educate students about programming autonomous robots using ROS (Robot Operating System) and Python software. The instructors wanted students to feel comfortable with their technical skills so that they could enjoy this camp and pursue further educational opportunities with clear fundamentals. Students hope to spread their knowledge and help others transition into a STEM disciple.  \r\n\r\nInstructors taught students how to use the robot’s laser sensor to detect range from obstacles, camera to identify the color of obstacles, odometry to perceive the robot’s movement, and a combination of these sensors to determine where it is in a map. Not only did they learn the theory behind these concepts from MIT and JPL staff, they were able to implement it on robots by following instructions on labs and learn from a trial and error basis.  \r\n\r\nStudents were also able to experience seminar talks about cutting-edge technology from experts in their fields daily. They learnt about topics ranging from designing hackathons to creating mini-satellites to even getting some insider tips into the admissions process at MIT!\r\n\r\n\r\n# Technical Learning for Each Week\r\n## Week 1\r\nThe goal for week one was to become familiar with Linux and ROS, and then begin to control the driving of the racecar, eventually leading to a wall-following program. We began by learning about basic Linux commands and ROS structure. We familiarized ourselves with Linux commands to create directories and navigate through files, and learned about the modularity of ROS. \r\n\r\nModularity is an important aspect of ROS because it enables the user to use nodes and libraries created by others without fully understanding the details. A single task is divided into smaller, simpler tasks, each accomplished by a node, which work together to accomplish the larger goal. The modularity of ROS enables members of a team to work independently and simultaneously at first, each on a different node, and then join together after to put all the nodes together. We learned how to create ROS nodes, including publishers and subscribers, allowing the nodes to communicate and interact with each other. \r\n\r\nRobotic control systems were the main theme of this week. We learned about open-loop versus closed-loop controls, and about bang-bang and PID controls. In an open loop system, the system’s output does not affect later outputs. In a closed-loop system, however, the system’s output is fed back into sensors, which send these values back to the controller and change future outputs. The robot uses sensors to learn about its own current state, or a state estimation. In order to make a wall follower algorithm, we would need to take data from sensors on the racecar, specifically the lidar, and use this data to change the racecar’s behavior. Lidar, or laser radar, is a sensor that sends out lasers and detects the returning laser signals in order to calculate distance. The Lidar on the racecar extends 270 degrees around the car, from right to left, and returns a list of 1080 data points for distance, 4 for each degree. \r\n\r\nWe began with a bang-bang algorithm to make the racecar wall follow. Bang-bang control the simplest type of control, enabling the system to only alternate between two states. There is a set threshold, and if the error is outside of the threshold, some action is done to increase or decrease the error. Bang-bang control, while it is simple, is not usually the best type of control because the system is constantly switching between two extremes, and thus will oscillate significantly depending on the starting state estimation. \r\nWe used the race car's lidar data and took the minimum distance on the side of the wall the racecar is supposed to follow. Using this distance, we calculated our error (error = d_desired - distance) and compared this error to our set threshold. If the absolute value of error was less than the threshold , then the racecar would continue to drive straight. If not, the racecar would either steer right or left in order to maintain its error within the threshold. \r\nWe then improved our bang-bang controller to use an average distance instead of the minimum distance from the lidar. We took a small sector of the lidar data and then found the average of those distances to get a new estimated distance, which we then based our error on and used to control the racecar’s steering. \r\nA PID controller is another closed-loop control system similar to bang-bang, but more sophisticated so that the car does not oscillate as much. PID stands for Proportional Integral Derivative, which shows how the control system accounts for past, present, and future error values. A PID controller is more effective and refined than a bang-bang controller because it has three coefficient variables, Kp (proportional), Ki (integral), and Kd (derivative). It adjusts the system’s response based on how large or small the error is, how strong previous outputs were, and the current rate of error change. \r\n\r\nWe began developing a more sophisticated control system by starting with just the Kp coefficient. The addition of this coefficient makes the system output proportional to the error. If the error is large, the racecar steering is also large, if the error is small, the racecar steering is small. This enables the racecar to get as close to equilibrium as possible, as opposed to constantly oscillating between two extremes. We later added a Kd coefficient to make the wall follower even more effective, as it would alter the output to match previous outputs. \r\nDuring the challenge, our team’s racecar successfully wall followed without crashing, but we did not have the fastest time. Especially when put to the far right or left, the racecar would create large oscillations in trying to balance its error within the threshold. Our Kp and Kd values were not refined enough to minimize these wide oscillations. Even though it was not the most efficient, at least our wall follower worked. After all, having a safe car is much more important than having a fast car. \r\n\r\n## Week 2\r\nTechnical Goals: This week we hoped to be comfortable with the basic software and therefore program more. We wanted to get better at image processing, \r\n\r\nProcess: We started the week by learning about the opencv software, which allows the ZED Camera to read images and the processor to process images and extract meaningful information such as color, size, etc. We learnt how to convert from the RGB color system (one we are familiar with) to the HSV color system (one ideal for the computer). Some functions we used were the inRange function which gave us all the pixels which were within our bounds and bitwise_and which allowed the image’s selected colors to be published. We then worked with the ZED Camera and recorded its data using rosbag. We modified given code to detect and distinguish a red light from a green light.\r\nOn the next day, we created a publisher and subscriber node for the images of the ZED Camera. The subscriber node subscribed to images sent on the /camera/rgb/image_rect_color topic and the publisher node published image messages to  image_echo. \r\n\r\nOn Wednesday, we had to design a control system that would take in information from the ZED Camera to stop in front of the green blob. We had to make two nodes. The first one would detect the green blob by converting the zed camera image to something that ROS can use. Then, we make a mask (which separates the green bits from other color bits we don’t need).We use function that filter out noise such as Gaussian Blur. After that, we draw contours around the largest mask, and we follow the contour. Once the mask reaches a certain pixel size, we’d know whether we were close enough and then we would stop.\r\n\r\nOn Thursday and Friday, we had to design a robot that could follow either the left or right wall based on the color it sees. To do this, we first saw the blob and recognized whether it was green or blue. Then, we used a P controller to navigate towards it. Once we got close enough to the blob, we turned right or left (based on the color) and switched to our wall following program.\r\n\r\nOur robot was unsuccessful because we did not develop a reliable program which could transition between the node that went towards the blob and the node that started following the wall. Moreover, our robot could not always find the proper blob because it was often confused by the colors of the clothing the spectators were wearing and confused by the colors of the flag.\r\n\r\nTo try to fix these errors, we increased the threshold for the pixel length of the blob and narrowed down the color ranges so that it would only find the blob. To add a quick transition between programs, we modified our wall following program to turn right if x was pressed and no wall was found, and to turn left if a was pressed and no wall was found. Once the wall was found, we would go back to our PD controlled wall follower. However, we did not get time to test these changes.\r\n\r\n## Week 3\r\n\r\nWeek 3, while not the capstone week, introduced higher level autonomy while integrating the technologies taught in Weeks 1 and 2. We students were expected to use blob and color detection along with space exploration algorithms more complex than Week 1 wall-following to explore the given space. Localization and mapping - robot location sensing - was also a large focus of Week 3, and the technical lectures included path planning algorithms given a map of the robot space. \r\n\r\nMapping and localization, introduced toward the beginning of the week, is integral to autonomous robotics. Indeed, robot mobility requires knowledge of robot location. Autonomous robots, however, cannot accurately track their location and direction with mere GPS and compass; robots need to employ other sensing techniques to map their local environments and describe a path. This is where localization and mapping plays a role. Our RCs can use the lidar data detected from multiple positions along with knowledges of the distance and orientation of the RC itself (from the odometer) to map their environments. Instead of first mapping then localizing or vice versa, the robots use SLAM, or simultaneous localization and mapping, whereby localization occurs while the map is concurrently made. \r\nThe SLAM task at hand was simple: map the local race environment while also localizing the car with odometry. The RC used a built in program to run the SLAM algorithm, but the generated map lacked clarity and precision. The map seemed to fold in on itself like a pretzel, suggesting that the odometry measurements used to predict the direction and speed of the racecar were inaccurate. With further techniques, we could have perfected the map by accounting for odometry inaccuracies. \r\n\r\nPast localization and mapping, Week 3 also introduced vehicle mobility robotics such as path planning and obstacle avoidance algorithms. During the technical lectures, we learned about discretization of spaces and the application of A* and RQT algorithms, and during the lab, we implemented a variety of obstacle avoidance algorithms. The original obstacle avoidance algorithms written simply moved the car to the nearest “white space” available; the car would move towards the points with the farthest lidar readings. One major problem faced this week was the inaccuracies of certain Lidar measurements. Our group used an averaging algorithm to essentially smooth out the data generated by the lidar and prevent from inaccurate measurements. \r\n\r\nLater in the week, the potential fields method was introduced, whereby the vehicle is considered a “positive charge”, each particle is considered a “positive charge”, and the subsequent potential field is produced and analyzed. Following Winters’ explanation, our group derived a slightly more complex version of the algorithm giving us more control over vehicle turning. After much tuning, we produced an algorithm that could effectively avoid obstacles in a smooth, seamless manner. The robustness of this potential fields algorithm convinced us to use it again for the final race in Week 4 (see Week 4).\r\n\r\nThe final technical challenge in Week 3 was, in reality, a combination of Week 1 and Week 2: we were expected to \r\nexplore the open space while simultaneously using image detection to detect certain colors. The key difference in image analysis between Week 2 and Week 3, however, was the new image detection requirement; we would earn more points detecting real images (that had been provided beforehand to the computer) within pink paper. Our group originally tackled this problem by computing the histograms of both the input image stored in the computer and the image from the ZED camera and comparing the two. However, we soon realized that the lighting conditions were subject to change, throwing off the histograms and skewing the image detection. Our team was unfortunately unable to solve this problem during Week 3, but we switched over to feature detection as opposed to histogram comparison during Week 4. \r\nOur final challenge ended up subpar, unfortunately - the team hit 8 obstacles, and we recognized merely 1 image. We soon learned after the race that our blue detection had been switched with the red detection (so even though we detected the colors, we didn’t correctly identify the colors), and upon further scrutiny, we realized that within the code, our HSV to BGR conversion had been mistakenly HSV to RGB (thus switching the red and the blue). This was a major blow to our team and our confidence, but we shook it off and prepared for the challenges in Week 4.\r\n\r\n##Week 4\r\nThe idea behind week 4 was to take all the concepts that we had learned in the previous 3 weeks and compile them into one project so that we could show off what was learned. A racetrack was set up in MIT’s Walker Memorial and we students were tasked with moving the robots along the track in an attempt to achieve the fastest time. \r\nDuring the first two days of Week 4 we students prepared for the technical challenge. On the first day our group prepared for the obstacle avoiding challenge, which was very similar to the one we did on Week 3. Our robot was supposed to go around an open space with obstacles and pick up as many blobs as possible without avoiding the obstacles. The first thing our group did was we re-wrote the code from week 3 over the weekends. It was very helpful because it made testing and debugging a lot easier; the new code also made it easier to add new color filters to detect new colors. By writing new codes, our group also discovered the BGR RGB mistake last week. The challenge of obstacle avoiding mission remained as our car continued to collide. Our solutions was to make the car less responsive to obstacles on the side. Our group tuned the constants in our potential field program and successfully avoided all the obstacles. \r\n\r\nOn Tuesday we had to deal with making a colored turn. Instead of hard coding the turn like in week 2, our group used a modified version of potential field to control the turn. When our car approached the wall, our code produced an ‘illusionary’ wall on one side of the car. The side was determined by the color we picked up on the wall. The car would then try to avoid the obstacles - the wall in front as well as the imaginary wall we created, and naturally it made a turn in our desired direction. \r\n\r\nOn Wednesday our group went to Walker Memorial and prepared for the final race. In the morning while people were setting up the race track our group worked on our codes. We delegated tasks into three parts: Normal racing, which focused on staying straight and avoiding the walls, Turning, which focused on making the correct turn at the shortcut intersection, and Controlling, which evaluated current conditions and made decisions on whether the car should use normal racing strategy or it should use shortcut handling strategy. The day didn’t go well for us, however. Every part of our code had problems. The racing strategy code could not make the car go in a straight line and avoid walls. The turning code made the car crash into the wall, and the Controlling code couldn’t switch from turning to racing at the right time. During the night our group decided to merge the racing and turning code, using the strategy of an imaginary wall like we used for our technical challenge. We quickly wrote a new code. \r\n\r\nOn Thursday we realized we were in a time crunch, but we still took the time and made a plan for the day. In the morning we focused on racing. We added a D controller to our potential field code, which adjusted steering angle based on how drastically the potential had been changing. The D controller was nice because whenever there was a drastic change it brought the steering angle down, which avoided the car constantly turning on a straight course. During the day we kept adjusting our control system, and by the end of the day our car could finish the race smoothly. We also made a team poster with a team name. \r\n\r\nOn Friday our group raced well. Despite having a lot of hardware issues our car had one of the strongest finishes as it finished all three of the time trial. \r\n\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}